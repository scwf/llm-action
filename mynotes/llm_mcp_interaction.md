# LLM 如何自动调用 MCP 服务器

在 MCP 架构中，LLM（大语言模型）与 MCP 服务器的交互是一个精心设计的过程。让我详细解释 LLM 是如何知道何时以及如何调用 MCP 服务器的。

## LLM 与 MCP 服务器的交互机制

### 1. MCP 客户端作为中间层

首先，需要理解的是，LLM 并不是直接与 MCP 服务器通信的。实际上，有一个 MCP 客户端作为中间层：

```
用户 → LLM 应用（如Claude Desktop）→ MCP 客户端 → MCP 服务器
```

Claude Desktop 等应用内置了 MCP 客户端功能，负责管理与 MCP 服务器的连接。

### 2. 服务器注册与发现

当你使用 `mcp install weather_server.py` 命令时，发生了以下步骤：

1. MCP CLI 工具解析你的服务器代码
2. 它提取服务器的元数据（名称、描述、资源、工具和提示）
3. 这些信息被注册到 Claude Desktop 的服务器注册表中
4. Claude Desktop 现在"知道"有一个名为"天气助手"的服务器可用，以及它提供的所有功能

### 3. 服务器能力声明

在初始化过程中，MCP 服务器会声明其能力（capabilities）：

```python
# 这是在底层自动发生的
capabilities = {
    "resources": {"list": True},
    "tools": {"list": True},
    "prompts": {"list": True},
    # 其他能力...
}
```

这些能力告诉 LLM 应用这个服务器可以提供哪些功能。

## LLM 如何决定调用 MCP 服务器

### 1. 上下文注入

当用户与 Claude 交互时，Claude Desktop 会在提交给 LLM 的上下文中注入关于已安装 MCP 服务器的信息：

```
[系统信息：你有权访问以下工具和资源：
- 天气助手：提供天气信息和预报
  - 资源：weather://{city} - 获取城市天气
  - 工具：forecast_weather(city, days) - 预测未来天气
  - 工具：convert_temperature(celsius) - 温度转换
]
```

这些信息是在用户看不到的系统提示中注入的。

### 2. LLM 理解与决策

LLM（如 Claude）被训练为理解这些系统提示，并知道如何使用这些工具和资源。当用户问一个问题时，LLM 会：

1. 分析用户的问题
2. 检查可用的工具和资源
3. 决定是否需要调用 MCP 服务器
4. 如果需要，确定调用哪个工具或资源

例如，当用户问"北京今天的天气怎么样？"时：

- Claude 识别这是关于天气的问题
- 它注意到有一个"天气助手"服务器提供天气信息
- 它看到有一个 `weather://{city}` 资源可以获取城市天气
- 它决定调用这个资源，将 `{city}` 替换为 "北京"

### 3. 工具调用协议

当 LLM 决定调用 MCP 服务器时，它会生成一个特殊的响应格式，表明它想要调用工具：

```json
{
  "tool_calls": [
    {
      "id": "call_abc123",
      "type": "function",
      "function": {
        "name": "get_city_weather",
        "arguments": {
          "uri": "weather://北京"
        }
      }
    }
  ]
}
```

或者对于工具调用：

```json
{
  "tool_calls": [
    {
      "id": "call_def456",
      "type": "function",
      "function": {
        "name": "forecast_weather",
        "arguments": {
          "city": "上海",
          "days": 5
        }
      }
    }
  ]
}
```

### 4. 客户端处理工具调用

Claude Desktop（或其他 MCP 客户端）会拦截这些工具调用请求：

1. 解析工具调用请求
2. 确定需要调用哪个 MCP 服务器
3. 建立与服务器的连接（如果尚未连接）
4. 发送适当的 MCP 协议消息（如 `readResource` 或 `callTool`）
5. 等待服务器响应
6. 将响应返回给 LLM

### 5. LLM 整合结果

LLM 接收到工具调用的结果后，会将其整合到回答中：

1. 解析工具返回的数据
2. 理解数据的含义
3. 将数据转化为自然语言回答
4. 可能会添加额外的解释或建议

## 具体例子：完整流程

让我们详细跟踪一个完整的交互流程：

1. **用户提问**：用户在 Claude Desktop 中输入"北京今天的天气怎么样？"

2. **请求处理**：
   - Claude Desktop 将用户问题和系统提示（包含 MCP 服务器信息）发送给 Claude

3. **LLM 分析**：
   - Claude 分析问题，识别这是关于北京天气的查询
   - Claude 注意到系统提示中有一个天气助手服务器
   - Claude 决定使用 `weather://北京` 资源

4. **工具调用请求**：
   - Claude 生成一个工具调用请求，要求读取 `weather://北京` 资源
   - 这个请求被发送回 Claude Desktop

5. **客户端处理**：
   - Claude Desktop 识别这是一个资源读取请求
   - 它连接到天气助手 MCP 服务器
   - 它发送 MCP `readResource` 消息，URI 为 `weather://北京`

6. **服务器处理**：
   - 天气助手服务器接收到请求
   - 它调用 `get_city_weather("北京")` 函数
   - 函数返回天气信息，如"北京的天气：晴朗，温度：25°C"
   - 服务器将结果发送回客户端

7. **结果返回**：
   - Claude Desktop 接收到天气信息
   - 它将这个信息作为工具调用的结果返回给 Claude

8. **LLM 整合**：
   - Claude 接收到天气信息
   - 它将这个信息整合到一个自然语言回答中
   - 可能会添加一些建议，如"这是一个适合户外活动的好天气"

9. **用户看到回答**：
   - 用户看到 Claude 的完整回答，包含北京的天气信息和相关建议

## 关键技术点

1. **系统提示注入**：LLM 通过系统提示了解可用的 MCP 服务器和功能
2. **工具调用协议**：LLM 使用标准化的工具调用格式请求外部数据
3. **MCP 协议**：客户端和服务器之间使用 MCP 协议通信
4. **无缝集成**：对用户来说，整个过程是无缝的，就像 LLM 本身知道天气一样

这种架构的优势在于：

- **分离关注点**：LLM 专注于理解和生成，MCP 服务器专注于提供数据和功能
- **实时数据**：LLM 可以访问最新数据，而不是仅限于训练数据
- **扩展能力**：通过添加新的 MCP 服务器，可以轻松扩展 LLM 的能力
- **标准化**：使用标准协议，使不同的 LLM 和服务器可以互操作

通过这种方式，MCP 架构使 LLM 能够"知道自己不知道什么"，并知道如何获取所需的信息或功能。 